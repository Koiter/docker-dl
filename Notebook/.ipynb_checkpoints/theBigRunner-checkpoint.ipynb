{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3303 images belonging to 5 classes.\n",
      "Found 367 images belonging to 5 classes.\n",
      "Image batch shape:  (32, 32, 32, 3)\n",
      "Label batch shape:  (32, 5)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tf.VERSION\n",
    "\n",
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\n",
    "train_data = image_generator.flow_from_directory(\"flower_photos\",target_size=(32,32))\n",
    "validation_data = image_generator.flow_from_directory(\"flower_photos_test\",target_size=(32,32))\n",
    "\n",
    "for image_batch,label_batch in train_data:\n",
    "  print(\"Image batch shape: \", image_batch.shape)\n",
    "  print(\"Label batch shape: \", label_batch.shape)\n",
    "  break\n",
    "\n",
    "class CollectBatchStats(tf.keras.callbacks.Callback):\n",
    "  def __init__(self):\n",
    "    self.batch_losses = []\n",
    "    self.batch_acc = []\n",
    "    self.epoch_val_loss = []\n",
    "    self.epoch_val_acc = []\n",
    "    \n",
    "  def on_batch_end(self, batch, logs=None):\n",
    "    self.batch_losses.append(logs['loss'])\n",
    "    self.batch_acc.append(logs['acc'])\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    self.epoch_val_loss.append(logs['val_loss'])\n",
    "    self.epoch_val_acc.append(logs['val_acc'])\n",
    "\n",
    "runs = [2, 3]\n",
    "activation = [0, 0, 1, 1, 2, 2]\n",
    "lossF = ['categorical_crossentropy', 'mean_squared_error', 'categorical_crossentropy', 'mean_squared_error', 'categorical_crossentropy', 'mean_squared_error']\n",
    "saveName = ['Tanh', 'MSETanh', 'RELU', 'MSERELU', 'LeakyRELU', 'MSELeakyRELU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in runs:\n",
    "    model = None\n",
    "    if (activation[id] == 0):\n",
    "        # CNN 2\n",
    "        model = tf.keras.Sequential([\n",
    "          layers.Conv2D(32, [3, 3], (1, 1), 'valid', input_shape=(image_batch.shape[1], image_batch.shape[2], image_batch.shape[3])),\n",
    "          layers.Activation('tanh'),\n",
    "          # layers.LeakyReLU(alpha=0.3),\n",
    "          layers.MaxPooling2D(pool_size=(2 , 2)),\n",
    "\n",
    "          layers.Conv2D(64, [3, 3], (1, 1), 'valid'),\n",
    "          layers.Activation('tanh'),\n",
    "          # layers.LeakyReLU(alpha=0.3),\n",
    "          layers.MaxPooling2D(pool_size=(2 , 2)),\n",
    "\n",
    "          layers.Conv2D(128, [3, 3], (1, 1), 'valid'),\n",
    "          layers.Activation('tanh'),\n",
    "          # layers.LeakyReLU(alpha=0.3),\n",
    "          layers.Flatten(),\n",
    "\n",
    "          layers.Dense(train_data.num_classes, activation='softmax')\n",
    "        ])\n",
    "    if (activation[id] == 1):\n",
    "        # CNN 2\n",
    "        model = tf.keras.Sequential([\n",
    "          layers.Conv2D(32, [3, 3], (1, 1), 'valid', input_shape=(image_batch.shape[1], image_batch.shape[2], image_batch.shape[3])),\n",
    "          layers.Activation('relu'),\n",
    "          # layers.LeakyReLU(alpha=0.3),\n",
    "          layers.MaxPooling2D(pool_size=(2 , 2)),\n",
    "\n",
    "          layers.Conv2D(64, [3, 3], (1, 1), 'valid'),\n",
    "          layers.Activation('relu'),\n",
    "          # layers.LeakyReLU(alpha=0.3),\n",
    "          layers.MaxPooling2D(pool_size=(2 , 2)),\n",
    "\n",
    "          layers.Conv2D(128, [3, 3], (1, 1), 'valid'),\n",
    "          layers.Activation('relu'),\n",
    "          # layers.LeakyReLU(alpha=0.3),\n",
    "          layers.Flatten(),\n",
    "\n",
    "          layers.Dense(train_data.num_classes, activation='softmax')\n",
    "        ])\n",
    "    if (activation[id] == 2):\n",
    "        # CNN 2\n",
    "        model = tf.keras.Sequential([\n",
    "          layers.Conv2D(32, [3, 3], (1, 1), 'valid', input_shape=(image_batch.shape[1], image_batch.shape[2], image_batch.shape[3])),\n",
    "          # layers.Activation('relu'),\n",
    "          layers.LeakyReLU(alpha=0.3),\n",
    "          layers.MaxPooling2D(pool_size=(2 , 2)),\n",
    "\n",
    "          layers.Conv2D(64, [3, 3], (1, 1), 'valid'),\n",
    "          # layers.Activation('relu'),\n",
    "          layers.LeakyReLU(alpha=0.3),\n",
    "          layers.MaxPooling2D(pool_size=(2 , 2)),\n",
    "\n",
    "          layers.Conv2D(128, [3, 3], (1, 1), 'valid'),\n",
    "          # layers.Activation('relu'),\n",
    "          layers.LeakyReLU(alpha=0.3),\n",
    "          layers.Flatten(),\n",
    "\n",
    "          layers.Dense(train_data.num_classes, activation='softmax')\n",
    "        ])\n",
    "    model.compile(\n",
    "        optimizer=tf.train.AdamOptimizer(), \n",
    "        loss=lossF[id],\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    batch_stats = CollectBatchStats()\n",
    "    model.fit_generator(train_data, epochs=4, \n",
    "        callbacks = [batch_stats],\n",
    "        validation_data = validation_data)\n",
    "    \n",
    "    results = pd.DataFrame({\"losses\":batch_stats.batch_losses, \"accuracy\":batch_stats.batch_acc})\n",
    "    results.to_csv(\"saved_results/cnn\"+saveName[id]+\"Data.csv\", ',')\n",
    "    results = pd.DataFrame({\"losses\":batch_stats.epoch_val_loss, \"accuracy\":batch_stats.epoch_val_acc})\n",
    "    results.to_csv(\"saved_results/cnn\"+saveName[id]+\"ValData.csv\", ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-ce281ab9e58a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcategories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprocessed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "categories = len(np.unique(data.y_train))\n",
    "processed_data = preprocess(data, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3303 images belonging to 5 classes.\n",
      "Found 367 images belonging to 5 classes.\n",
      "Image batch shape:  (32, 32, 32, 1)\n",
      "Label batch shape:  (32, 5)\n",
      "This is run: 1\n",
      "\n",
      " \n",
      "\n",
      "Image size: 32 * 32\n",
      "Loss function: mean_squared_error\n",
      "Activation function: softmax\n",
      "\n",
      "\n",
      "12/12 [==============================] - 2s 204ms/step - loss: 0.1595 - acc: 0.2452\n",
      "104/104 [==============================] - 21s 199ms/step - loss: 0.1597 - acc: 0.2428 - val_loss: 0.1595 - val_acc: 0.2452\n",
      "This is run: 2\n",
      "\n",
      " \n",
      "\n",
      "Image size: 32 * 32\n",
      "Loss function: mean_squared_error\n",
      "Activation function: relu\n",
      "\n",
      "\n",
      "12/12 [==============================] - 2s 194ms/step - loss: 0.1534 - acc: 0.3161\n",
      "104/104 [==============================] - 21s 202ms/step - loss: 0.1925 - acc: 0.2419 - val_loss: 0.1534 - val_acc: 0.3161\n",
      "This is run: 3\n",
      "\n",
      " \n",
      "\n",
      "Image size: 32 * 32\n",
      "Loss function: mean_squared_error\n",
      "Activation function: leaky\n",
      "\n",
      "\n",
      "12/12 [==============================] - 2s 194ms/step - loss: 0.3108 - acc: 0.2180\n",
      "104/104 [==============================] - 21s 198ms/step - loss: 0.3097 - acc: 0.2195 - val_loss: 0.3108 - val_acc: 0.2180\n"
     ]
    }
   ],
   "source": [
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\n",
    "actFunc = ['softmax','relu','leaky','tanh']\n",
    "lossFunc = ['mean_squared_error','categorical_crossentropy']\n",
    "imSizes = [32,64]\n",
    "\n",
    "def getData(size):\n",
    "    t_data = image_generator.flow_from_directory(\"flower_photos\",color_mode=\"grayscale\",target_size=(size,size))\n",
    "    v_data = image_generator.flow_from_directory(\"flower_photos_test\",color_mode=\"grayscale\",target_size=(size,size))\n",
    "    return t_data, v_data\n",
    "\n",
    "def plainVanilla(images, labels, activ, los):\n",
    "    #print (images.shape[1], images.shape[2], images.shape[3])\n",
    "    if actFunc[k] == 'leaky':\n",
    "        model = tf.keras.Sequential([\n",
    "            layers.Flatten(input_shape=(images.shape[1], images.shape[2], images.shape[3])),\n",
    "            layers.Dense(1024),\n",
    "            layers.LeakyReLU(alpha=0.3),\n",
    "            layers.Dense(512),\n",
    "            layers.LeakyReLU(alpha=0.3),\n",
    "            layers.Dense(labels, activation='softmax')\n",
    "        ])\n",
    "    else:\n",
    "        model = tf.keras.Sequential([\n",
    "            layers.Flatten(input_shape=(images.shape[1], images.shape[2], images.shape[3])),\n",
    "            layers.Dense(1024, activation=activ),\n",
    "            #layers.Dropout(0.2),\n",
    "            layers.Dense(512, activation=activ),\n",
    "            #layers.Dropout(0.2),\n",
    "            layers.Dense(labels, activation='softmax')\n",
    "        ])\n",
    "    model.compile(\n",
    "        optimizer=tf.train.AdamOptimizer(), \n",
    "        loss=los,\n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def trainModel(model, tr_dat, val_dat):\n",
    "    batch_stats = CollectBatchStats()\n",
    "    model.fit_generator(tr_dat, epochs=epoch, \n",
    "        callbacks = [batch_stats],         \n",
    "        validation_data = val_dat)\n",
    "    return callbacks\n",
    "\n",
    "    \n",
    "\n",
    "#initialisation\n",
    "epoch = 1\n",
    "\n",
    "savedMods = []\n",
    "runs = 1\n",
    "for i in range((len(imSizes))):\n",
    "    train_dat, validation_dat = getData(imSizes[i])\n",
    "    \n",
    "    for image_batch,label_batch in train_dat:\n",
    "        print(\"Image batch shape: \", image_batch.shape)\n",
    "        print(\"Label batch shape: \", label_batch.shape)\n",
    "        break\n",
    "    \n",
    "    for j in range(len(lossFunc)):\n",
    "        for k in range(len(actFunc)):\n",
    "            print ('This is run: '+ str(runs))\n",
    "            print ('\\n \\n')\n",
    "            print ('Image size: '+ str(imSizes[i]) + ' * ' + str(imSizes[i]))\n",
    "            print ('Loss function: '+lossFunc[j])\n",
    "            print ('Activation function: '+actFunc[k])\n",
    "            print ( '\\n')\n",
    "            model = plainVanilla(image_batch, train_dat.num_classes, actFunc[k], lossFunc[j])\n",
    "            batch_stats = CollectBatchStats()\n",
    "            mod = model.fit(train_dat,\n",
    "                            epochs=epoch, \n",
    "                            validation_data=validation_dat,\n",
    "                            callbacks=[batch_stats]\n",
    "                           )\n",
    "            savedMods.append([imSizes[i], actFunc[k], lossFunc[j], mod])\n",
    "            runs += 1\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# extra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['roses', 'sunflowers', 'daisy', 'dandelion', 'tulips']\n"
     ]
    }
   ],
   "source": [
    "# pak van elke soort een plaatje\n",
    "soorten = os.listdir('flower_photos')[0:5]\n",
    "simg = []\n",
    "for n in range(5):\n",
    "    simg.append(Image('flower_photos/'+soorten[n]+'/'+os.listdir('flower_photos/'+soorten[n])[5]))\n",
    "print(soorten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)\n",
    "len(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3303 images belonging to 5 classes.\n",
      "Found 367 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\n",
    "train_data = image_generator.flow_from_directory(\"flower_photos\",target_size=(32,32))\n",
    "validation_data = image_generator.flow_from_directory(\"flower_photos_test\",target_size=(32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0\n",
      "0 0 1\n",
      "0 0 2\n",
      "0 0 3\n",
      "0 1 0\n",
      "0 1 1\n",
      "0 1 2\n",
      "0 1 3\n",
      "1 0 0\n",
      "1 0 1\n",
      "1 0 2\n",
      "1 0 3\n",
      "1 1 0\n",
      "1 1 1\n",
      "1 1 2\n",
      "1 1 3\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
