{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3303 images belonging to 5 classes.\n",
      "Found 367 images belonging to 5 classes.\n",
      "Image batch shape:  (32, 32, 32, 3)\n",
      "Label batch shape:  (32, 5)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import optimizers \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "tf.VERSION\n",
    "\n",
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\n",
    "train_data = image_generator.flow_from_directory(\"flower_photos\",target_size=(32,32))\n",
    "validation_data = image_generator.flow_from_directory(\"flower_photos_test\",target_size=(32,32))\n",
    "\n",
    "for image_batch,label_batch in train_data:\n",
    "  print(\"Image batch shape: \", image_batch.shape)\n",
    "  print(\"Label batch shape: \", label_batch.shape)\n",
    "  break\n",
    "\n",
    "class CollectBatchStats(tf.keras.callbacks.Callback):\n",
    "  def __init__(self):\n",
    "    self.batch_losses = []\n",
    "    self.batch_acc = []\n",
    "    self.epoch_val_loss = []\n",
    "    self.epoch_val_acc = []\n",
    "    \n",
    "  def on_batch_end(self, batch, logs=None):\n",
    "    self.batch_losses.append(logs['loss'])\n",
    "    self.batch_acc.append(logs['acc'])\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    self.epoch_val_loss.append(logs['val_loss'])\n",
    "    self.epoch_val_acc.append(logs['val_acc'])\n",
    "\n",
    "runs = [2, 3]\n",
    "activation = [0, 0, 1, 1, 2, 2]\n",
    "lossF = ['categorical_crossentropy', 'mean_squared_error', 'categorical_crossentropy', 'mean_squared_error', 'categorical_crossentropy', 'mean_squared_error']\n",
    "saveName = ['Tanh', 'MSETanh', 'RELU', 'MSERELU', 'LeakyRELU', 'MSELeakyRELU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in runs:\n",
    "    model = None\n",
    "    if (activation[id] == 0):\n",
    "        # CNN 2\n",
    "        model = tf.keras.Sequential([\n",
    "          layers.Conv2D(32, [3, 3], (1, 1), 'valid', input_shape=(image_batch.shape[1], image_batch.shape[2], image_batch.shape[3])),\n",
    "          layers.Activation('tanh'),\n",
    "          # layers.LeakyReLU(alpha=0.3),\n",
    "          layers.MaxPooling2D(pool_size=(2 , 2)),\n",
    "\n",
    "          layers.Conv2D(64, [3, 3], (1, 1), 'valid'),\n",
    "          layers.Activation('tanh'),\n",
    "          # layers.LeakyReLU(alpha=0.3),\n",
    "          layers.MaxPooling2D(pool_size=(2 , 2)),\n",
    "\n",
    "          layers.Conv2D(128, [3, 3], (1, 1), 'valid'),\n",
    "          layers.Activation('tanh'),\n",
    "          # layers.LeakyReLU(alpha=0.3),\n",
    "          layers.Flatten(),\n",
    "\n",
    "          layers.Dense(train_data.num_classes, activation='softmax')\n",
    "        ])\n",
    "    if (activation[id] == 1):\n",
    "        # CNN 2\n",
    "        model = tf.keras.Sequential([\n",
    "          layers.Conv2D(32, [3, 3], (1, 1), 'valid', input_shape=(image_batch.shape[1], image_batch.shape[2], image_batch.shape[3])),\n",
    "          layers.Activation('relu'),\n",
    "          # layers.LeakyReLU(alpha=0.3),\n",
    "          layers.MaxPooling2D(pool_size=(2 , 2)),\n",
    "\n",
    "          layers.Conv2D(64, [3, 3], (1, 1), 'valid'),\n",
    "          layers.Activation('relu'),\n",
    "          # layers.LeakyReLU(alpha=0.3),\n",
    "          layers.MaxPooling2D(pool_size=(2 , 2)),\n",
    "\n",
    "          layers.Conv2D(128, [3, 3], (1, 1), 'valid'),\n",
    "          layers.Activation('relu'),\n",
    "          # layers.LeakyReLU(alpha=0.3),\n",
    "          layers.Flatten(),\n",
    "\n",
    "          layers.Dense(train_data.num_classes, activation='softmax')\n",
    "        ])\n",
    "    if (activation[id] == 2):\n",
    "        # CNN 2\n",
    "        model = tf.keras.Sequential([\n",
    "          layers.Conv2D(32, [3, 3], (1, 1), 'valid', input_shape=(image_batch.shape[1], image_batch.shape[2], image_batch.shape[3])),\n",
    "          # layers.Activation('relu'),\n",
    "          layers.LeakyReLU(alpha=0.3),\n",
    "          layers.MaxPooling2D(pool_size=(2 , 2)),\n",
    "\n",
    "          layers.Conv2D(64, [3, 3], (1, 1), 'valid'),\n",
    "          # layers.Activation('relu'),\n",
    "          layers.LeakyReLU(alpha=0.3),\n",
    "          layers.MaxPooling2D(pool_size=(2 , 2)),\n",
    "\n",
    "          layers.Conv2D(128, [3, 3], (1, 1), 'valid'),\n",
    "          # layers.Activation('relu'),\n",
    "          layers.LeakyReLU(alpha=0.3),\n",
    "          layers.Flatten(),\n",
    "\n",
    "          layers.Dense(train_data.num_classes, activation='softmax')\n",
    "        ])\n",
    "    model.compile(\n",
    "        optimizer=tf.train.AdamOptimizer(), \n",
    "        loss=lossF[id],\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    batch_stats = CollectBatchStats()\n",
    "    model.fit_generator(train_data, epochs=4, \n",
    "        callbacks = [batch_stats],\n",
    "        validation_data = validation_data)\n",
    "    \n",
    "    results = pd.DataFrame({\"losses\":batch_stats.batch_losses, \"accuracy\":batch_stats.batch_acc})\n",
    "    results.to_csv(\"saved_results/cnn\"+saveName[id]+\"Data.csv\", ',')\n",
    "    results = pd.DataFrame({\"losses\":batch_stats.epoch_val_loss, \"accuracy\":batch_stats.epoch_val_acc})\n",
    "    results.to_csv(\"saved_results/cnn\"+saveName[id]+\"ValData.csv\", ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "results = pandas.DataFrame({\"losses\":batch_stats.batch_losses, \"accuracy\":batch_stats.batch_acc})\n",
    "results.to_csv(\"saved_results/cnn\"+saveName[id]+\"Data.csv\", ',')\n",
    "results = pandas.DataFrame({\"losses\":batch_stats.epoch_val_loss, \"accuracy\":batch_stats.epoch_val_acc})\n",
    "results.to_csv(\"saved_results/cnn\"+saveName[id]+\"ValData.csv\", ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple as nt\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "Data = nt(\"Data\", \"x_train y_train x_valid y_valid x_test y_test\")\n",
    "%matplotlib inline\n",
    "def preprocess(data, categories):\n",
    "    x_train = data.x_train.astype(\"float32\") / 255\n",
    "    x_test = data.x_test.astype(\"float32\") / 255\n",
    "    y_train = to_categorical(data.y_train, categories)\n",
    "    y_test = to_categorical(data.y_test, categories)    \n",
    "    return Data(x_train[5000:], y_train[5000:],\n",
    "                x_train[:5000], y_train[:5000],\n",
    "                x_test, y_test)\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "data = Data(x_train, y_train, None, None, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = len(np.unique(data.y_train))\n",
    "processed_data = preprocess(data, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "def getData(size):\n",
    "    t_data = image_generator.flow_from_directory(\"flower_photos\",target_size=(size,size))\n",
    "    v_data = image_generator.flow_from_directory(\"flower_photos_test\",target_size=(size,size))\n",
    "    return t_data, v_data\n",
    "\n",
    "def plainVanilla(images, labels, activ, los):\n",
    "    print (images.shape[1], images.shape[2], images.shape[3])\n",
    "    model = tf.keras.Sequential([\n",
    "      layers.Flatten(input_shape=(images.shape[1], images.shape[2], images.shape[3])),\n",
    "      #layers.Flatten(input_shape=(images.x_train.shape[1:])),\n",
    "      layers.Dense((images.shape[1]*images.shape[2]*images.shape[3])/2, activation=activ),\n",
    "      #layers.Dropout(0.2),\n",
    "      layers.Dense((images.shape[1]*images.shape[2]*images.shape[3])/4, activation=activ),\n",
    "      #layers.Dropout(0.2),\n",
    "      #layers.Dense(128, activation=activ),\n",
    "      #layers.Dropout(0.2),\n",
    "      layers.Dense(labels, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.train.AdamOptimizer(), \n",
    "        loss=los,\n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def trainModel(model, tr_dat, val_dat):\n",
    "    batch_stats = CollectBatchStats()\n",
    "    model.fit_generator(tr_dat, epochs=5, \n",
    "        callbacks = [batch_stats],         \n",
    "        validation_data = val_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3303 images belonging to 5 classes.\n",
      "Found 367 images belonging to 5 classes.\n",
      "Image batch shape:  (32, 32, 32, 3)\n",
      "Label batch shape:  (32, 5)\n",
      "32 32 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1536)              4720128   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 768)               1180416   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 3845      \n",
      "=================================================================\n",
      "Total params: 5,904,389\n",
      "Trainable params: 5,904,389\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "12/12 [==============================] - 3s 210ms/step - loss: 1.3947 - acc: 0.4196\n",
      "104/104 [==============================] - 21s 204ms/step - loss: 1.8742 - acc: 0.3500 - val_loss: 1.3947 - val_acc: 0.4196\n",
      "Epoch 2/5\n",
      "12/12 [==============================] - 2s 178ms/step - loss: 1.2884 - acc: 0.4005\n",
      "104/104 [==============================] - 19s 183ms/step - loss: 1.2756 - acc: 0.4366 - val_loss: 1.2884 - val_acc: 0.4005\n",
      "Epoch 3/5\n",
      "12/12 [==============================] - 2s 187ms/step - loss: 1.3502 - acc: 0.4196\n",
      "104/104 [==============================] - 18s 177ms/step - loss: 1.2122 - acc: 0.4759 - val_loss: 1.3502 - val_acc: 0.4196\n",
      "Epoch 4/5\n",
      "12/12 [==============================] - 2s 190ms/step - loss: 1.2471 - acc: 0.4550\n",
      "104/104 [==============================] - 20s 193ms/step - loss: 1.1619 - acc: 0.5104 - val_loss: 1.2471 - val_acc: 0.4550\n",
      "Epoch 5/5\n",
      "12/12 [==============================] - 2s 180ms/step - loss: 1.3859 - acc: 0.4305\n",
      "104/104 [==============================] - 19s 180ms/step - loss: 1.1201 - acc: 0.5322 - val_loss: 1.3859 - val_acc: 0.4305\n",
      "Found 3303 images belonging to 5 classes.\n",
      "Found 367 images belonging to 5 classes.\n",
      "Image batch shape:  (32, 64, 64, 3)\n",
      "Label batch shape:  (32, 5)\n",
      "64 64 3\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 12288)             0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6144)              75503616  \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 3072)              18877440  \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5)                 15365     \n",
      "=================================================================\n",
      "Total params: 94,396,421\n",
      "Trainable params: 94,396,421\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "12/12 [==============================] - 3s 220ms/step - loss: 12.8945 - acc: 0.1907\n",
      "104/104 [==============================] - 44s 420ms/step - loss: 12.9675 - acc: 0.1895 - val_loss: 12.8945 - val_acc: 0.1907\n",
      "Epoch 2/5\n",
      "12/12 [==============================] - 3s 235ms/step - loss: 12.9896 - acc: 0.1907\n",
      "104/104 [==============================] - 42s 399ms/step - loss: 13.0715 - acc: 0.1904 - val_loss: 12.9896 - val_acc: 0.1907\n",
      "Epoch 3/5\n",
      "12/12 [==============================] - 3s 228ms/step - loss: 12.9896 - acc: 0.1907\n",
      "104/104 [==============================] - 42s 403ms/step - loss: 13.0544 - acc: 0.1904 - val_loss: 12.9896 - val_acc: 0.1907\n",
      "Epoch 4/5\n",
      "12/12 [==============================] - 3s 238ms/step - loss: 12.9896 - acc: 0.1907\n",
      "104/104 [==============================] - 42s 403ms/step - loss: 13.0715 - acc: 0.1904 - val_loss: 12.9896 - val_acc: 0.1907\n",
      "Epoch 5/5\n",
      "12/12 [==============================] - 3s 218ms/step - loss: 13.0848 - acc: 0.1907\n",
      "104/104 [==============================] - 44s 421ms/step - loss: 13.0544 - acc: 0.1904 - val_loss: 13.0848 - val_acc: 0.1907\n"
     ]
    }
   ],
   "source": [
    "i = 32\n",
    "while i <= 64:\n",
    "    train_dat, validation_dat = getData(i)\n",
    "    for image_batch,label_batch in train_dat:\n",
    "        print(\"Image batch shape: \", image_batch.shape)\n",
    "        print(\"Label batch shape: \", label_batch.shape)\n",
    "        break\n",
    "    mlp = plainVanilla(image_batch, train_dat.num_classes, 'relu', lossF[0])\n",
    "    mlp.summary()\n",
    "    #mlp = plainVanilla(processed_data, categories, 'relu', lossF[0])\n",
    "    trainModel(mlp, train_dat, validation_dat)\n",
    "    i *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3303 images belonging to 5 classes.\n",
      "Found 367 images belonging to 5 classes.\n",
      "Image batch shape:  (32, 32, 32, 1)\n",
      "Label batch shape:  (32, 5)\n",
      "32 32 1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_3 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 657,413\n",
      "Trainable params: 657,413\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "12/12 [==============================] - 2s 195ms/step - loss: 1.5251 - acc: 0.2997\n",
      "104/104 [==============================] - 21s 202ms/step - loss: 1.5948 - acc: 0.2810 - val_loss: 1.5251 - val_acc: 0.2997\n",
      "Epoch 2/5\n",
      "12/12 [==============================] - 2s 192ms/step - loss: 1.5941 - acc: 0.2834\n",
      "104/104 [==============================] - 18s 173ms/step - loss: 1.5396 - acc: 0.3025 - val_loss: 1.5941 - val_acc: 0.2834\n",
      "Epoch 3/5\n",
      "12/12 [==============================] - 2s 166ms/step - loss: 1.5318 - acc: 0.2698\n",
      "104/104 [==============================] - 18s 172ms/step - loss: 1.5162 - acc: 0.3246 - val_loss: 1.5318 - val_acc: 0.2698\n",
      "Epoch 4/5\n",
      "12/12 [==============================] - 2s 169ms/step - loss: 1.5300 - acc: 0.3324\n",
      "104/104 [==============================] - 17s 159ms/step - loss: 1.4862 - acc: 0.3309 - val_loss: 1.5300 - val_acc: 0.3324\n",
      "Epoch 5/5\n",
      "12/12 [==============================] - 2s 182ms/step - loss: 1.5958 - acc: 0.3324\n",
      "104/104 [==============================] - 19s 178ms/step - loss: 1.4632 - acc: 0.3694 - val_loss: 1.5958 - val_acc: 0.3324\n",
      "Found 3303 images belonging to 5 classes.\n",
      "Found 367 images belonging to 5 classes.\n",
      "Image batch shape:  (32, 64, 64, 1)\n",
      "Label batch shape:  (32, 5)\n",
      "64 64 1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_4 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2048)              8390656   \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 1024)              2098176   \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 5)                 5125      \n",
      "=================================================================\n",
      "Total params: 10,493,957\n",
      "Trainable params: 10,493,957\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "12/12 [==============================] - 2s 201ms/step - loss: 12.0550 - acc: 0.2452\n",
      "104/104 [==============================] - 22s 211ms/step - loss: 12.0530 - acc: 0.2440 - val_loss: 12.0550 - val_acc: 0.2452\n",
      "Epoch 2/5\n",
      "12/12 [==============================] - 3s 213ms/step - loss: 12.0550 - acc: 0.2452\n",
      "104/104 [==============================] - 21s 204ms/step - loss: 12.1703 - acc: 0.2446 - val_loss: 12.0550 - val_acc: 0.2452\n",
      "Epoch 3/5\n",
      "12/12 [==============================] - 2s 180ms/step - loss: 12.0550 - acc: 0.2452\n",
      "104/104 [==============================] - 20s 190ms/step - loss: 12.1874 - acc: 0.2446 - val_loss: 12.0550 - val_acc: 0.2452\n",
      "Epoch 4/5\n",
      "12/12 [==============================] - 3s 214ms/step - loss: 12.0550 - acc: 0.2452: 0s - los\n",
      "104/104 [==============================] - 21s 204ms/step - loss: 12.1703 - acc: 0.2446 - val_loss: 12.0550 - val_acc: 0.2452\n",
      "Epoch 5/5\n",
      "12/12 [==============================] - 2s 196ms/step - loss: 12.0550 - acc: 0.2452\n",
      "104/104 [==============================] - 21s 202ms/step - loss: 12.1703 - acc: 0.2446 - val_loss: 12.0550 - val_acc: 0.2452\n"
     ]
    }
   ],
   "source": [
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "def getData(size):\n",
    "    t_data = image_generator.flow_from_directory(\"flower_photos\",color_mode=\"grayscale\",target_size=(size,size))\n",
    "    v_data = image_generator.flow_from_directory(\"flower_photos_test\",color_mode=\"grayscale\",target_size=(size,size))\n",
    "    return t_data, v_data\n",
    "\n",
    "def plainVanilla(images, labels, activ, los):\n",
    "    print (images.shape[1], images.shape[2], images.shape[3])\n",
    "    model = tf.keras.Sequential([\n",
    "      layers.Flatten(input_shape=(images.shape[1], images.shape[2], images.shape[3])),\n",
    "      #layers.Flatten(input_shape=(images.x_train.shape[1:])),\n",
    "      layers.Dense((images.shape[1]*images.shape[2]*images.shape[3])/2, activation=activ),\n",
    "      #layers.Dropout(0.2),\n",
    "      layers.Dense((images.shape[1]*images.shape[2]*images.shape[3])/4, activation=activ),\n",
    "      #layers.Dropout(0.2),\n",
    "      #layers.Dense(128, activation=activ),\n",
    "      #layers.Dropout(0.2),\n",
    "      layers.Dense(labels, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=tf.train.AdamOptimizer(), \n",
    "        loss=los,\n",
    "        metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def trainModel(model, tr_dat, val_dat):\n",
    "    batch_stats = CollectBatchStats()\n",
    "    model.fit_generator(tr_dat, epochs=5, \n",
    "        callbacks = [batch_stats],         \n",
    "        validation_data = val_dat)\n",
    "\n",
    "i = 32\n",
    "while i <= 64:\n",
    "    train_dat, validation_dat = getData(i)\n",
    "    for image_batch,label_batch in train_dat:\n",
    "        print(\"Image batch shape: \", image_batch.shape)\n",
    "        print(\"Label batch shape: \", label_batch.shape)\n",
    "        break\n",
    "    mlp = plainVanilla(image_batch, train_dat.num_classes, 'relu', lossF[0])\n",
    "    mlp.summary()\n",
    "    #mlp = plainVanilla(processed_data, categories, 'relu', lossF[0])\n",
    "    trainModel(mlp, train_dat, validation_dat)\n",
    "    i *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['roses', 'sunflowers', 'daisy', 'dandelion', 'tulips']\n"
     ]
    }
   ],
   "source": [
    "# pak van elke soort een plaatje\n",
    "soorten = os.listdir('flower_photos')[0:5]\n",
    "simg = []\n",
    "for n in range(5):\n",
    "    simg.append(Image('flower_photos/'+soorten[n]+'/'+os.listdir('flower_photos/'+soorten[n])[5]))\n",
    "print(soorten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
